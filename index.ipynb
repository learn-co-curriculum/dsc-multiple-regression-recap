{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Multiple Linear Regression - Recap\n", "\n", "## Introduction\n", "\n", "In this section you extended your knowledge of building regression models by adding additional predictive variables, including categorical variables."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Key Takeaways\n", "\n", "* ***Multiple linear regression*** builds on simple linear regression\n", "  * Instead of finding just one coefficient and one intercept, multiple regression finds a coefficient for each independent variable (predictor)\n", "  * The coefficients are not the same if you build several different simple linear regression models vs. using those same predictors in a single multiple regression model. This is because a multiple regression model is \"***controlling for***\" the other variables.\n", "    * A randomized controlled experiment is the gold standard way to control for confounding variables, but multiple regression is an alternative strategy when an experiment is not possible\n", "* In an inferential context, we typically use ***StatsModels*** to build multiple regression models, just like we did for simple regression models\n", "  * Another library that can be used for linear regression is ***scikit-learn***. This is mostly relevant for predictive modeling (machine learning) because it doesn't calculate p-values for coefficients\n", "* There are some problems with R-Squared (coefficient of determination) for evaluating multiple regression models\n", "  * R-Squared will only ever increase as we add more features. ***Adjusted R-Squared*** accounts for the number of features and is a better metric for multiple regression\n", "  * Proportion of variance explained is not intuitive for stakeholders. ***Error-based metrics*** such as mean absolute error (MAE) and root mean squared error (RMSE) are helpful tools to express how incorrect the model is in an average prediction\n", "* Now that we have the ability to use multiple features in our regression models, we can use ***categorical features***\n", "  * Categorical features must be ***preprocessed*** before they can be used in linear regression models\n", "  * Specifically we used ***one-hot encoding*** to create dummy variables (1s or 0s) representing each category\n", "  * In order to avoid the ***dummy variable trap*** we need to drop one of the dummy variable columns. Whichever column is dropped becomes the ***reference category***, where all other category coefficients are compared to this category."]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 2}